loss: 1   #index from possible_loss list
# possible loss list needs to be updated both here and in code in model_gen_util.py in pymodconn
possible_loss: [cvrmse_loss, 'mse', 'mae', 'msle', 'mape', 'cosine_similarity', 'logcosh', hourly_MSE, hourly_MAE, hourly_MSLE, hourly_MAPE, hourly_huber_loss, cs_huber_loss]
quantiles: [0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99]
metrics: [mape, mse, mae, msle]
optimizer: Adam
Adam:
  lr: 0.001
  b1: 0.9
  b2: 0.999
  epsi: 1.0e-08
SGD:
  lr: 0.001
  momentum: 0.9
save_models_dir: saved_models/

if_seed: 0
seed: 500

training:
  # 1: normal training, 2: training with CustomEarlyStopping(CES), 3: training with multiple runs and best model selection,  4: transfer learning 
  training_type: 2  
  # veriables for training_type 3
  prelim_runs: 8
  prelim_runs_epochs: 10

  IF_MULTIPROCESS: 1       # 0: MULTIPROCESS window is not created, 1: MULTIPROCESS window is used
  IF_TRAIN_AT_ALL: 1
  num_of_same_run: 1
  
  LOAD_or_TRAIN_MODEL: 'TRAIN'           # 'LOAD': load model, 'TRAIN': train model
  LOAD_MODEL_PATH: saved_results/RNN_CIT2.yaml_07.06.2023-15.14.05.055_runNUMBER-1_modelcheckpoint_best.h5
  IF_multipleGPU: 0
  epochs: 50
  batch_size: 512
  seq_len: 20
  ifLRS: 1
  ifEarlyStop: 0
  EarlyStop_patience: 25
  EarlyStop_min_delta: 0.001
  EarlyStop_start_from_epoch: 20
  training_verbose: 2
  if_validation: 1

#settings for transfer learning
tl_case:
  # start and end if tl_case runs is split into multiple runs
  start: 0   # full run is 0 to 27 
  end: 1

  tl_type: 2
  # 1: full model is fine tuned, 
  # 2: 'some' layer is fine tuned, 
  # 3: 'some' layer is fresh trained, 
  # 4: Extra layer is added and trained
  # 5: 'some' layer is fine tuned, then extra layer is added and trained
  # 6: Extra layer is added and trained and then 'some' layer is fine tuned

  # if tl_type is 2 or 3, then 'some' layer type
  some_layer: [22]
  #enc: all encoder layers
  #dec: all decoder layers
  #0 - encoder_past_inputs True
  #1 - decoder_1_inputs True
  #2 - dense True
  #3 - dense_10 True
  #4 - dropout True
  #5 - dropout_3 True
  #6 - bidirectional True
  #7 - bidirectional_1 True
  #8 - linear_layer True
  #9 - linear_layer_5 True
  #10 - encoder_1_selfMHA-1 True
  #11 - decoder_1_selfMHA-1 True
  #12 - glu_with_addnorm True
  #13 - glu_with_addnorm_2 True
  #14 - grn_layer True
  #15 - decoder_1_crossMHA-1 True
  #16 - glu_with_addnorm_3 True
  #17 - concatenate True
  #18 - grn_layer_1 True
  #19 - linear_layer_11 True
  #20 - bidirectional_2 True
  #21 - linear_layer_12 True
  #22 - time_distributed_23 True
  #23 - merge_list True
  #24 - lambda True
  #25 - activation_2 True

  # if tl_type is 4, then extra layer type
  extra_layer_type: GRU                   # Dense-relu, Dense-sigmoid, Dense-tanh, LSTM, GRU, MHA
  
  # training parameters, i.e. if tl_type is 3 or 4
  epochs: 50
  batch_size: 512
  lr: 0.001
  optimizer: Adam

  # if tl_type is 5 or 6. 
  # For tl_type:5 Step one 5-1 is tuning model with tl_5_layer, then 5-2 is adding extra layer with tl_5_extra_layer_type
  tl_5_1_layer: [22]
  tl_5_1_lr: 0.0001
  tl_5_1_optimizer: Adam
  tl_5_1_epochs: 50
  tl_5_1_batch_size: 512

  tl_5_current: tune

  tl_5_2_extra_layer_type: GRU
  tl_5_2_epochs: 75
  tl_5_2_batch_size: 512
  tl_5_2_lr: 0.005         # lr for CWR_max_lr
  tl_5_2_optimizer: Adam


CustomEarlyStopping:
  max_training_attempts: 25
  check_epoch_index: 3    #epoch starts from 0
  min_error: 25
  error_type: mse

CosineWarmRestart:
  CWR_min_lr: 0.0001
  CWR_max_lr: 0.005
  CWR_steps_per_epoch: 35
  CWR_lr_decay: 0.85
  CWR_cycle_length: 6
  CWR_cycle_mult_factor: 1
  CWR_warmup_length: 2
  CWR_warmup_mult_factor: 1